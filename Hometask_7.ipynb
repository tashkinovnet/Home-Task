{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuMFD93Drc/h+pmqslsaCQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tashkinovnet/Home-Task/blob/Hometask_7.ipynb/Hometask_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8PHI8AKp5Ly"
      },
      "source": [
        "#**Hometask 7**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9ZpBC7Sgauk"
      },
      "source": [
        "###Задача: запустить модель LDA и Gibbs Sampling с числов тегов 20. Вывести топ-10 слов по каждому тегу. Соотнести полученные теги с тегами из датасета. Добейтесь того, чтобы хотя бы несколько тем были явно интерпретируемы, например, как в примерах ниже.\n",
        "\n",
        "Примеры топ-10 слов из некотрых тегов, которые получаются после применения LDA:\n",
        "* ['god', 'jesus', 'believe', 'life', 'bible', 'christian', 'world', 'church', 'word', 'people'] - эта группа явно соотносится с soc.religion.christian\n",
        "* ['drive', 'card', 'hard', 'bit', 'disk', 'scsi', 'memory', 'speed', 'mac', 'video'] - эту группу можно соотнести с темами 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware'\n",
        "* ['game',\t'games',\t'hockey',\t'league',\t'play',\t'players',\t'season',\t'team',\t'teams',\t'win'] - тема rec.sport.hockey\n",
        "\n",
        "Советы:\n",
        "* модель будет сходится лучше и быстрее, если уменьшить размер словаря за счет отсеивания общеупотребительных слов и редких слов. Управлять размером словаря можно с помощью параметров min_df (отсеивает слова по минимальной частоте встречаемости) и max_df (отсеивает слова по максимальной частоте встречаемости) в CountVectorizer.\n",
        "* параметры $\\alpha$, $\\beta$ можно, для начала, положить единицами\n",
        "* после 100 итераций можно ожидать хорошего распределения по темам. Если этого не происходит и в темах мешинина - проверяйте код и оптимизируйте словарь\n",
        "* на примере третьей темы видно, что у нас встречаются разные формы одного и того же слова. С помощью процедур stemming и lemmatization можно привести слова к общей форме и объединить близкие по значению"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJojZEDZ1TEN",
        "outputId": "2be7b61f-67cb-4747-afea-99d13429b5ca"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic #1: one, would, like, get, also, see, could, thing, time, say\n",
            "Topic #2: make, one, would, good, like, use, need, want, also, look\n",
            "Topic #3: one, like, never, know, people, much, something, thing, going, would\n",
            "Topic #4: god, believe, christian, say, people, word, jesus, life, must, bible\n",
            "Topic #5: government, key, use, public, law, system, chip, used, encryption, phone\n",
            "Topic #6: less, since, often, case, quite, cause, problem, level, certain, may\n",
            "Topic #7: think, would, know, want, even, people, say, much, get, one\n",
            "Topic #8: use, using, set, file, function, line, following, read, number, several\n",
            "Topic #9: window, system, use, using, card, drive, work, running, run, program\n",
            "Topic #10: like, one, get, would, know, come, still, also, use, look\n",
            "Topic #11: first, year, day, went, took, last, thought, said, told, month\n",
            "Topic #12: anyone, please, know, thanks, looking, would, post, email, send, reply\n",
            "Topic #13: state, people, government, right, american, force, member, world, war, child\n",
            "Topic #14: one, time, would, get, know, people, could, want, still, think\n",
            "Topic #15: would, like, get, one, think, probably, much, lot, make, better\n",
            "Topic #16: game, last, team, year, play, player, win, first, best, better\n",
            "Topic #17: information, available, space, general, list, university, research, include, system, send\n",
            "Topic #18: new, car, used, price, sell, buy, pay, power, work, cost\n",
            "Topic #19: way, take, want, right, kind, let, tell, put, come, always\n",
            "Topic #20: get, make, go, see, one, good, time, think, take, even\n",
            "Topic #1: rec.sport.baseball (86 documents)\n",
            "Top words: one, would, like, get, also, see, could, thing, time, say\n",
            "\n",
            "Topic #2: sci.electronics (49 documents)\n",
            "Top words: make, one, would, good, like, use, need, want, also, look\n",
            "\n",
            "Topic #3: rec.motorcycles (49 documents)\n",
            "Top words: one, like, never, know, people, much, something, thing, going, would\n",
            "\n",
            "Topic #4: soc.religion.christian (302 documents)\n",
            "Top words: god, believe, christian, say, people, word, jesus, life, must, bible\n",
            "\n",
            "Topic #5: sci.crypt (256 documents)\n",
            "Top words: government, key, use, public, law, system, chip, used, encryption, phone\n",
            "\n",
            "Topic #6: sci.med (89 documents)\n",
            "Top words: less, since, often, case, quite, cause, problem, level, certain, may\n",
            "\n",
            "Topic #7: rec.motorcycles (39 documents)\n",
            "Top words: think, would, know, want, even, people, say, much, get, one\n",
            "\n",
            "Topic #8: comp.windows.x (142 documents)\n",
            "Top words: use, using, set, file, function, line, following, read, number, several\n",
            "\n",
            "Topic #9: comp.sys.ibm.pc.hardware (258 documents)\n",
            "Top words: window, system, use, using, card, drive, work, running, run, program\n",
            "\n",
            "Topic #10: rec.motorcycles (39 documents)\n",
            "Top words: like, one, get, would, know, come, still, also, use, look\n",
            "\n",
            "Topic #11: rec.motorcycles (48 documents)\n",
            "Top words: first, year, day, went, took, last, thought, said, told, month\n",
            "\n",
            "Topic #12: comp.graphics (103 documents)\n",
            "Top words: anyone, please, know, thanks, looking, would, post, email, send, reply\n",
            "\n",
            "Topic #13: talk.politics.mideast (246 documents)\n",
            "Top words: state, people, government, right, american, force, member, world, war, child\n",
            "\n",
            "Topic #14: rec.motorcycles (38 documents)\n",
            "Top words: one, time, would, get, know, people, could, want, still, think\n",
            "\n",
            "Topic #15: rec.autos (36 documents)\n",
            "Top words: would, like, get, one, think, probably, much, lot, make, better\n",
            "\n",
            "Topic #16: rec.sport.hockey (280 documents)\n",
            "Top words: game, last, team, year, play, player, win, first, best, better\n",
            "\n",
            "Topic #17: sci.space (102 documents)\n",
            "Top words: information, available, space, general, list, university, research, include, system, send\n",
            "\n",
            "Topic #18: misc.forsale (175 documents)\n",
            "Top words: new, car, used, price, sell, buy, pay, power, work, cost\n",
            "\n",
            "Topic #19: rec.motorcycles (34 documents)\n",
            "Top words: way, take, want, right, kind, let, tell, put, come, always\n",
            "\n",
            "Topic #20: rec.motorcycles (28 documents)\n",
            "Top words: get, make, go, see, one, good, time, think, take, even\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "documents = newsgroups_train.data\n",
        "labels = newsgroups_train.target\n",
        "label_names = newsgroups_train.target_names\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(doc):\n",
        "    words = [lemmatizer.lemmatize(word.lower()) for word in doc.split() if word.isalpha() and word.lower() not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "preprocessed_documents = [preprocess_text(doc) for doc in documents]\n",
        "vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000)\n",
        "X = vectorizer.fit_transform(preprocessed_documents)\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "\n",
        "n_topics = 20\n",
        "alpha = 1.0\n",
        "beta = 1.0\n",
        "n_iterations = 100\n",
        "\n",
        "n_docs, n_words = X.shape\n",
        "word_topic_counts = np.zeros((n_words, n_topics))\n",
        "doc_topic_counts = np.zeros((n_docs, n_topics))\n",
        "topic_totals = np.zeros(n_topics)\n",
        "doc_topic_assignments = []\n",
        "\n",
        "for d in range(n_docs):\n",
        "    doc = X[d].indices\n",
        "    topics = []\n",
        "    for w in doc:\n",
        "        topic = random.randint(0, n_topics - 1)\n",
        "        word_topic_counts[w, topic] += 1\n",
        "        doc_topic_counts[d, topic] += 1\n",
        "        topic_totals[topic] += 1\n",
        "        topics.append(topic)\n",
        "    doc_topic_assignments.append(topics)\n",
        "\n",
        "# Gibbs Sampling\n",
        "for it in range(n_iterations):\n",
        "    for d in range(n_docs):\n",
        "        doc = X[d].indices\n",
        "        for i, w in enumerate(doc):\n",
        "            current_topic = doc_topic_assignments[d][i]\n",
        "            word_topic_counts[w, current_topic] -= 1\n",
        "            doc_topic_counts[d, current_topic] -= 1\n",
        "            topic_totals[current_topic] -= 1\n",
        "\n",
        "            topic_probs = (word_topic_counts[w] + beta) * (doc_topic_counts[d] + alpha) / (topic_totals + beta * n_words)\n",
        "            topic_probs /= topic_probs.sum()\n",
        "\n",
        "            new_topic = np.random.choice(np.arange(n_topics), p=topic_probs)\n",
        "            word_topic_counts[w, new_topic] += 1\n",
        "            doc_topic_counts[d, new_topic] += 1\n",
        "            topic_totals[new_topic] += 1\n",
        "            doc_topic_assignments[d][i] = new_topic\n",
        "\n",
        "# Получение топ-слов для каждой темы\n",
        "def get_top_words(word_topic_counts, vocab, n_top_words=10):\n",
        "    topics = []\n",
        "    for topic_idx in range(n_topics):\n",
        "        top_words_idx = word_topic_counts[:, topic_idx].argsort()[::-1][:n_top_words]\n",
        "        topics.append([vocab[i] for i in top_words_idx])\n",
        "    return topics\n",
        "\n",
        "topics = get_top_words(word_topic_counts, vocab)\n",
        "\n",
        "for i, topic in enumerate(topics):\n",
        "    print(f\"Topic #{i + 1}: {', '.join(topic)}\")\n",
        "\n",
        "topic_assignments = np.argmax(doc_topic_counts, axis=1)\n",
        "topic_to_labels = {i: [] for i in range(n_topics)}\n",
        "\n",
        "for doc_idx, topic in enumerate(topic_assignments):\n",
        "    topic_to_labels[topic].append(labels[doc_idx])\n",
        "\n",
        "for topic_idx, label_list in topic_to_labels.items():\n",
        "    label_counts = np.bincount(label_list, minlength=len(label_names))\n",
        "    if label_counts.sum() > 0:\n",
        "        top_label_idx = label_counts.argmax()\n",
        "        print(f\"Topic #{topic_idx + 1}: {label_names[top_label_idx]} ({label_counts[top_label_idx]} documents)\")\n",
        "        print(f\"Top words: {', '.join(topics[topic_idx])}\\n\")\n"
      ]
    }
  ]
}