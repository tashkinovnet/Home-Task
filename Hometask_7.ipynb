{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8PHI8AKp5Ly"
      },
      "source": [
        "#**Hometask 7**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9ZpBC7Sgauk"
      },
      "source": [
        "###Задача: запустить модель LDA и Gibbs Sampling с числов тегов 20. Вывести топ-10 слов по каждому тегу. Соотнести полученные теги с тегами из датасета. Добейтесь того, чтобы хотя бы несколько тем были явно интерпретируемы, например, как в примерах ниже.\n",
        "\n",
        "Примеры топ-10 слов из некотрых тегов, которые получаются после применения LDA:\n",
        "* ['god', 'jesus', 'believe', 'life', 'bible', 'christian', 'world', 'church', 'word', 'people'] - эта группа явно соотносится с soc.religion.christian\n",
        "* ['drive', 'card', 'hard', 'bit', 'disk', 'scsi', 'memory', 'speed', 'mac', 'video'] - эту группу можно соотнести с темами 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware'\n",
        "* ['game',\t'games',\t'hockey',\t'league',\t'play',\t'players',\t'season',\t'team',\t'teams',\t'win'] - тема rec.sport.hockey\n",
        "\n",
        "Советы:\n",
        "* модель будет сходится лучше и быстрее, если уменьшить размер словаря за счет отсеивания общеупотребительных слов и редких слов. Управлять размером словаря можно с помощью параметров min_df (отсеивает слова по минимальной частоте встречаемости) и max_df (отсеивает слова по максимальной частоте встречаемости) в CountVectorizer.\n",
        "* параметры $\\alpha$, $\\beta$ можно, для начала, положить единицами\n",
        "* после 100 итераций можно ожидать хорошего распределения по темам. Если этого не происходит и в темах мешинина - проверяйте код и оптимизируйте словарь\n",
        "* на примере третьей темы видно, что у нас встречаются разные формы одного и того же слова. С помощью процедур stemming и lemmatization можно привести слова к общей форме и объединить близкие по значению"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJojZEDZ1TEN",
        "outputId": "58440c54-14dd-44a6-af44-01920ee0cf57"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Запуск Gibbs Sampling...\n",
            "Прогресс: 0/100 итераций выполнено\n",
            "Прогресс: 10/100 итераций выполнено\n",
            "Прогресс: 20/100 итераций выполнено\n",
            "Прогресс: 30/100 итераций выполнено\n",
            "Прогресс: 40/100 итераций выполнено\n",
            "Прогресс: 50/100 итераций выполнено\n",
            "Прогресс: 60/100 итераций выполнено\n",
            "Прогресс: 70/100 итераций выполнено\n",
            "Прогресс: 80/100 итераций выполнено\n",
            "Прогресс: 90/100 итераций выполнено\n",
            "Gibbs Sampling завершен.\n",
            "Topic #1: window, file, using, program, use, run, version, running, application, available\n",
            "Topic #2: people, state, right, government, law, country, fact, american, gun, war\n",
            "Topic #3: said, day, took, three, first, year, woman, left, found, child\n",
            "Topic #4: information, space, research, general, university, following, list, new, contact, number\n",
            "Topic #5: cause, result, may, different, less, since, level, must, effect, although\n",
            "Topic #6: drive, card, work, disk, computer, system, video, hard, mac, memory\n",
            "Topic #7: god, believe, say, christian, people, must, word, jesus, life, true\n",
            "Topic #8: one, much, used, use, two, way, time, work, found, thing\n",
            "Topic #9: would, like, get, know, want, one, think, something, someone, thing\n",
            "Topic #10: think, one, even, say, know, point, make, want, see, something\n",
            "Topic #11: would, get, like, could, even, think, one, make, see, know\n",
            "Topic #12: game, last, year, team, play, first, player, win, got, best\n",
            "Topic #13: think, make, thing, people, say, good, one, see, much, probably\n",
            "Topic #14: like, one, would, think, make, much, long, know, might, may\n",
            "Topic #15: key, system, chip, use, public, government, phone, encryption, law, using\n",
            "Topic #16: would, one, need, also, like, could, good, make, new, want\n",
            "Topic #17: got, get, like, time, look, know, really, good, take, still\n",
            "Topic #18: anyone, please, thanks, know, would, looking, send, help, post, anybody\n",
            "Topic #19: one, read, know, first, would, like, may, since, see, want\n",
            "Topic #20: get, new, would, good, money, buy, also, pay, make, price\n",
            "Topic #1: comp.windows.x (1569 documents)\n",
            "Top words: window, file, using, program, use, run, version, running, application, available\n",
            "\n",
            "Topic #2: talk.politics.mideast (747 documents)\n",
            "Top words: people, state, right, government, law, country, fact, american, gun, war\n",
            "\n",
            "Topic #3: talk.politics.guns (628 documents)\n",
            "Top words: said, day, took, three, first, year, woman, left, found, child\n",
            "\n",
            "Topic #4: sci.space (629 documents)\n",
            "Top words: information, space, research, general, university, following, list, new, contact, number\n",
            "\n",
            "Topic #5: sci.med (466 documents)\n",
            "Top words: cause, result, may, different, less, since, level, must, effect, although\n",
            "\n",
            "Topic #6: comp.sys.ibm.pc.hardware (968 documents)\n",
            "Top words: drive, card, work, disk, computer, system, video, hard, mac, memory\n",
            "\n",
            "Topic #7: soc.religion.christian (777 documents)\n",
            "Top words: god, believe, say, christian, people, must, word, jesus, life, true\n",
            "\n",
            "Topic #8: rec.motorcycles (489 documents)\n",
            "Top words: one, much, used, use, two, way, time, work, found, thing\n",
            "\n",
            "Topic #9: rec.sport.baseball (472 documents)\n",
            "Top words: would, like, get, know, want, one, think, something, someone, thing\n",
            "\n",
            "Topic #10: talk.politics.misc (421 documents)\n",
            "Top words: think, one, even, say, know, point, make, want, see, something\n",
            "\n",
            "Topic #11: comp.graphics (378 documents)\n",
            "Top words: would, get, like, could, even, think, one, make, see, know\n",
            "\n",
            "Topic #12: rec.sport.hockey (623 documents)\n",
            "Top words: game, last, year, team, play, first, player, win, got, best\n",
            "\n",
            "Topic #13: alt.atheism (404 documents)\n",
            "Top words: think, make, thing, people, say, good, one, see, much, probably\n",
            "\n",
            "Topic #14: sci.electronics (356 documents)\n",
            "Top words: like, one, would, think, make, much, long, know, might, may\n",
            "\n",
            "Topic #15: sci.crypt (416 documents)\n",
            "Top words: key, system, chip, use, public, government, phone, encryption, law, using\n",
            "\n",
            "Topic #16: comp.sys.mac.hardware (297 documents)\n",
            "Top words: would, one, need, also, like, could, good, make, new, want\n",
            "\n",
            "Topic #17: rec.autos (352 documents)\n",
            "Top words: got, get, like, time, look, know, really, good, take, still\n",
            "\n",
            "Topic #18: comp.os.ms-windows.misc (681 documents)\n",
            "Top words: anyone, please, thanks, know, would, looking, send, help, post, anybody\n",
            "\n",
            "Topic #19: talk.religion.misc (286 documents)\n",
            "Top words: one, read, know, first, would, like, may, since, see, want\n",
            "\n",
            "Topic #20: misc.forsale (355 documents)\n",
            "Top words: get, new, would, good, money, buy, also, pay, make, price\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "documents = newsgroups_train.data\n",
        "labels = newsgroups_train.target\n",
        "label_names = newsgroups_train.target_names\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(doc):\n",
        "    words = [lemmatizer.lemmatize(word.lower()) for word in doc.split() if word.isalpha() and word.lower() not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "preprocessed_documents = [preprocess_text(doc) for doc in documents]\n",
        "\n",
        "vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000)\n",
        "X = vectorizer.fit_transform(preprocessed_documents)\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "\n",
        "n_topics = 20\n",
        "alpha = 1.0\n",
        "beta = 1.0\n",
        "n_iterations = 100\n",
        "\n",
        "n_docs, n_words = X.shape\n",
        "word_topic_counts = np.zeros((n_words, n_topics))\n",
        "doc_topic_counts = np.zeros((n_docs, n_topics))\n",
        "topic_totals = np.zeros(n_topics)\n",
        "doc_topic_assignments = []\n",
        "\n",
        "for d in range(n_docs):\n",
        "    doc = X[d].indices\n",
        "    topics = []\n",
        "    for w in doc:\n",
        "        topic = random.randint(0, n_topics - 1)\n",
        "        word_topic_counts[w, topic] += 1\n",
        "        doc_topic_counts[d, topic] += 1\n",
        "        topic_totals[topic] += 1\n",
        "        topics.append(topic)\n",
        "    doc_topic_assignments.append(topics)\n",
        "\n",
        "# Gibbs Sampling\n",
        "print(\"Запуск Gibbs Sampling...\")\n",
        "for it in range(n_iterations):\n",
        "    if it % 10 == 0:\n",
        "        print(f\"Прогресс: {it}/{n_iterations} итераций выполнено\")\n",
        "    for d in range(n_docs):\n",
        "        doc = X[d].indices\n",
        "        for i, w in enumerate(doc):\n",
        "            current_topic = doc_topic_assignments[d][i]\n",
        "            word_topic_counts[w, current_topic] -= 1\n",
        "            doc_topic_counts[d, current_topic] -= 1\n",
        "            topic_totals[current_topic] -= 1\n",
        "\n",
        "            topic_probs = (word_topic_counts[w] + beta) * (doc_topic_counts[d] + alpha) / (topic_totals + beta * n_words)\n",
        "            topic_probs /= topic_probs.sum()\n",
        "\n",
        "            new_topic = np.random.choice(np.arange(n_topics), p=topic_probs)\n",
        "            word_topic_counts[w, new_topic] += 1\n",
        "            doc_topic_counts[d, new_topic] += 1\n",
        "            topic_totals[new_topic] += 1\n",
        "            doc_topic_assignments[d][i] = new_topic\n",
        "\n",
        "print(\"Gibbs Sampling завершен.\")\n",
        "\n",
        "# Получение топ-слов для каждой темы\n",
        "def get_top_words(word_topic_counts, vocab, n_top_words=10):\n",
        "    topics = []\n",
        "    for topic_idx in range(n_topics):\n",
        "        top_words_idx = word_topic_counts[:, topic_idx].argsort()[::-1][:n_top_words]\n",
        "        topics.append([vocab[i] for i in top_words_idx])\n",
        "    return topics\n",
        "\n",
        "topics = get_top_words(word_topic_counts, vocab)\n",
        "\n",
        "for i, topic in enumerate(topics):\n",
        "    print(f\"Topic #{i + 1}: {', '.join(topic)}\")\n",
        "\n",
        "topic_assignments = np.argmax(doc_topic_counts, axis=1)\n",
        "topic_to_labels = {i: [] for i in range(n_topics)}\n",
        "\n",
        "for doc_idx, topic in enumerate(topic_assignments):\n",
        "    topic_to_labels[topic].append(labels[doc_idx])\n",
        "\n",
        "used_labels = set()\n",
        "final_topic_labels = {}\n",
        "for topic_idx, label_list in topic_to_labels.items():\n",
        "    label_counts = np.bincount(label_list, minlength=len(label_names))\n",
        "    if label_counts.sum() > 0:\n",
        "        for label_idx in label_counts.argsort()[::-1]:\n",
        "            if label_idx not in used_labels:\n",
        "                used_labels.add(label_idx)\n",
        "                final_topic_labels[topic_idx] = label_idx\n",
        "                break\n",
        "\n",
        "unused_labels = set(range(len(label_names))) - used_labels\n",
        "for topic_idx in range(n_topics):\n",
        "    if topic_idx not in final_topic_labels and unused_labels:\n",
        "        final_topic_labels[topic_idx] = unused_labels.pop()\n",
        "\n",
        "# Вывод финального распределения\n",
        "for topic_idx, label_idx in final_topic_labels.items():\n",
        "    print(f\"Topic #{topic_idx + 1}: {label_names[label_idx]} ({len(topic_to_labels[topic_idx])} documents)\")\n",
        "    print(f\"Top words: {', '.join(topics[topic_idx])}\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpXACB8ZfDVPPtMmICUrKK"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}